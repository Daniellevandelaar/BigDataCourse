{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUXcdi3SwKKJ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIfsUg-hzqiL"
   },
   "source": [
    "If you have never used google colab before check out this documentation: https://colab.research.google.com/notebooks/intro.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "if41-JhqwWrx"
   },
   "source": [
    "# Task 0: Import necessary modules\n",
    "First, we will import the libraries needed to run the practical. In python, several toolboxes are regularly used: scipy (for maths), matplotlib (for plotting) and numpy (for arrays).\n",
    "All the code written in this script can be loaded using the play button on the left of the coding block.\n",
    "\n",
    "You will need to download the following data file and save it to your google drive\n",
    "\n",
    "[mill.mat](https://github.com/predictive-clinical-neuroscience/BigDataCourse/blob/main/data/mill.mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9OnOC8Yw2rTe"
   },
   "outputs": [],
   "source": [
    "# basic modules\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import scipy.linalg as la\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Load data\n",
    "\n",
    "We are going to perform a dimensionality reduction technique called principal component analysis (PCA). Let's perform PCA on some interesting data. Download the mill image from brightspace and upload it on the left hand side under files. Show it using the imshow() function:\n",
    "\n",
    "First, we need to get the data\n",
    "\n",
    "### Option 1: Download to your Google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for you to be able to save the necessary data to your google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# change dir to data on your google drive\n",
    "# change BMS85/data to where you saved mill.mat \n",
    "os.chdir('drive/My Drive/BMS85/data/') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Download directly from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://raw.githubusercontent.com/predictive-clinical-neuroscience/BigDataCourse/main/data/mill.mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can actually do the work... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "-mdmYVYV3FJu",
    "outputId": "23a12818-1d3c-4694-a781-c9e649bfde3e"
   },
   "outputs": [],
   "source": [
    "matstruct_contents = sio.loadmat('mill.mat')\n",
    "img = matstruct_contents['img']\n",
    "plt.figure()\n",
    "plt.imshow(img)\n",
    "plt.title('original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXtmWujktIvg"
   },
   "source": [
    "# Task 2: Perform PCA\n",
    "PCA is performed by an eigendecomposition of the covariance matrix. Putting the mathematics aside; first, we subtract the mean from our image. Then we calculate the covariance using the cov() function and find the eigenvalues and eigenvectors using the eig() function. For more details of each function look at the numpy documentation (https://numpy.org/doc/stable/reference/routines.linalg.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vuyGLsTksslK"
   },
   "outputs": [],
   "source": [
    "x = img\n",
    "m = np.mean(x,0)\n",
    "x = x - np.tile(m,[np.size(x,0),1])\n",
    "covmat = np.cov(x.T)\n",
    "[evals, evecs] = la.eig(covmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07GPTy_mtsWn"
   },
   "source": [
    "Now we compute the total percentage of variance explained by eigenvectors 1, 1:2, 1:3, ..., 1:n. Then plot the results against the number of eigenvectors that contributed to the total variance explained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "ocUE9iaQtrcA",
    "outputId": "84d25e02-864e-4151-84e9-1a0a0cc2f388"
   },
   "outputs": [],
   "source": [
    "pve = np.cumsum(evals)/np.sum(evals)*100\n",
    "plt.figure()\n",
    "plt.plot(range(0,np.size(evals)),pve)\n",
    "plt.xlabel('number of eigenvectors');\n",
    "plt.ylabel('percent variance explained');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTNCN2JXt3DM"
   },
   "source": [
    "Create a vector ‘keep’ that indicates which of the top eigenvectors to keep if we wish to explain at least 90% of the total variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XsKK5YrXt2TB"
   },
   "outputs": [],
   "source": [
    "keep = pve<90\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lod8k4pRuJNK"
   },
   "source": [
    "How many eigenvectors do we need to explain at least 90% of the variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "m0nM8aLHuId3",
    "outputId": "17029aef-67ec-4325-99aa-04d528081dc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "n = sum(keep)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KdQjAxOuTu0"
   },
   "source": [
    "Overwrite ‘evecs’ so that it contains only the eigenvectors indicated by ‘keep’ (this is the reduced PCA space):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ShRU9M2juS6b"
   },
   "outputs": [],
   "source": [
    "evecs = evecs[:,keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsnQJ0lUuYcQ"
   },
   "source": [
    "Compute the scores of x in the reduced PCA space by pre-multiplying the eigenvectors in evecs by x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KlvM0iLuX0L"
   },
   "outputs": [],
   "source": [
    "scores = np.dot(x, evecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIjIypG8uoTf"
   },
   "source": [
    "# Task 3: Dimensionality reduction\n",
    "Now we can reconstruct the image using the scores by post-multiplying them by the transpose of evecs and then adding the mean back again:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "tUAA-DRVulPE",
    "outputId": "3ec2c56e-6f30-433e-d56c-3e018fbac1b0"
   },
   "outputs": [],
   "source": [
    "y = np.dot(scores, evecs.T)\n",
    "y = y + np.tile(m,[np.size(y,0),1])\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(y)\n",
    "plt.title('compressed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mT0KrW6u_dR"
   },
   "source": [
    "As you can see we can reconstruct the original image pretty well using only a subset of the eigenvectors. This means that instead of needing all of the information in ‘img’ we can just store the information in ‘evecs’ and ‘scores’. Let’s see how much storage space this saves us and then compute the compression ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "HFpPUU2bu-3u",
    "outputId": "761105d0-60c2-4b92-c89d-6ffe11daaf30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['original image: 750.112 kb']\n",
      "['compressed image: 141.224 kb']\n",
      "['compression ratio: 81.1729448402372 %']\n"
     ]
    }
   ],
   "source": [
    "kb_img = sys.getsizeof(x)/1000\n",
    "print(['original image: ' + np.str(kb_img) + ' kb'])\n",
    "kb_y = (sys.getsizeof(scores) + sys.getsizeof(evecs))/1000\n",
    "print(['compressed image: ' + np.str(kb_y) + ' kb']);\n",
    "compr_pct = (1 - kb_y / kb_img) * 100\n",
    "print(['compression ratio: ' + np.str(compr_pct) + ' %'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of PCA_practical.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
